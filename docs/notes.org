* kafka commands
** get consumer offsets for a specific group
bin/kafka-consumer-groups.sh --bootstrap-server kafka-0.clusa.perfa.local.rkcy.net:9092 --group rkcy_rpg_edge__rkcy.rpg.edge.GENERAL.response.0001_0 --describe
** get all consumer offsets
bin/kafka-consumer-groups.sh --bootstrap-server kafka-0.clusa.perfa.local.rkcy.net:9092 --describe --all-groups
** get partition offsets
bin/kafka-get-offsets.sh --bootstrap-server kafka-2.clusa.perfa.local.rkcy.net:9092
* rpg enhancements
** Guilds associate with Characters through GuildMember table indicating membership priveleges
** GuildEvent table records which Characters participate in which GuildEvents and to what extent
** GuildLoyalty gets calculated daily by examining build event participation of all Characters
** Items have item types and get fleshed out
* platform enhancements
** IO System
In addition to Process and Storage, add an IO system
It would have it's own dedicated topics and consumers
Designed for long running IO tasks
Is it now called APEICS??
** Respect Timeout in ExecuteTxnSync
- Put the timeout as a field on ApecsTxn
- When processing txns, if this field is expired, immediately fail
- Generated storage steps must never have a timeout
- When this is in place, already timed out requests will drain quickly
- Add a timeout result code so these are easy to identify in error topics
** Daylight savings time
Think through ramifications when everything is in utc
** Handle all kafka log callbacks
Currently some stderr is happening on broker communication issues
** Datacenter awareness improvements
*** Newer idea, keeping below for context
- Runner starts distinct consumers in each DC, duplicating them against partitions
- Admin controls which ones should actually process messages, the consumers are DC aware
- Get rid of per consumer admin topic, and send control messages directly to process and storage topics
- Admin leader election still works with subscribe to "control" topic, partition 0 assigned is leader
*** Sumarry of new topics, details below
- No more "admin" topic
- platform - just platform updates
- producer - producer status messages
- control - running process messages
- watchdog - pings and acks between DC watchdogs
- applog - all application logs can be sent here instead of stdout
*** Admin should be DC aware
*** Starting jobs should explicitly specify DC
*** New "control" topic needed, no longer use admin topic for start/stop consumer messages
**** Should have 1 partition per DC
*** "admin" topic should be renamed "platform" and used soley for plaform updates and producer sync
- get all run related messages out of here and put in "control" topic
*** A watchdog process should be run in each DC
**** Has route to get to it over internet to test inbound
**** Pings periodically with keep alive, admin acks
**** Acks pings from admin
*** One admin per DC is run
**** Admins "subscribe" to control topic with consumer group
**** Admin that gets partition 0 is the leader
- Only the leader sends messages on control topic
- Only the leader manages producers and topic generational migration
- Only leader services api calls that can mutate system
- Secondaries can service read only calls, like reading the current platform config
**** If an admin fails, kafka will reassign with subscribe and a new leader may be in place
**** Must handle assignment callback, I think that's available with librdkafka
*** Runner changes
**** One runner is run per DC
**** runner consumer "assigns" to DC specific partition of control topic, so they only get control messages for their DC
**** Runner Interface
- Kill
- Stop
- Start
- IsRunning
- Wait
*** New topic called applog
- All logs read from running process are presented here
- Should be structured json logs in each message
- Logs should be formatted correctly to go straight into elasticsearch
- One partition per DC
- Apps should optionally produce logs straight to this topic
* consumer types
** Config consumers
- Always read latest, never commit offsets, upon start advance to latest offset and read
- E.g. platform, config
** Stateless read latest
- Always start latest, only consider new messages, never commit offsets
- E.g. control (non-admin tasks), producer, watch
** Full Subscribe
- Use subscribe
- Use partition 0 as leader election
- E.g. control (admin task)
* instance store dead zones
If there's ever a hard storage read caused by a cache miss, that
instance in the instance store must record an offset dead zone,
between the offset of the last storage write and the offset of the
refresh_instance from storage that replaces the item in the cache.

At the end of every APECS transaction, all instances that were updated
are considered, much like we do with generating UPDATE_ASYNC
commands. But instead of UPDATE_ASYNC we place a VALIDATE_INSTANCE
step at the end of the transaction for every instance that was
changed. VALIDATE_INSTANCE steps ensure that their offsets are not
within the dead zone.

After all VALIDATE_INSTANCE steps run, if any of them report a dead
zone violation, the transaction is rolled back.

During rollback, if any instance being rolled back violates the dead
zone, that step is not rolled back since its changes were never
applied to the instance store.
