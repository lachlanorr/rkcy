# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.

version: "3.8"

services:
  postgres:
    image: postgres
    restart: always
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_HOST_AUTH_METHOD: trust
      #POSTGRES_PASSWORD: CHANGEME

#  # Remove grafana for now, should add some dashboards for prometheus at some point
#  grafana:
#    image: grafana/grafana
#    restart: always
#    container_name: grafana
#    ports:
#      - "3000:3000"

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"

  zipkin:
    container_name: zipkin
    image: openzipkin/zipkin:latest
    ports:
      - "9411:9411"

  otel-collector:
    image: otel/opentelemetry-collector:latest
    container_name: otel-collector
    command: ["--config=/etc/otel/otel-collector.yaml"] #, "--log-level=DEBUG"]
    volumes:
      - ./otel-collector.yaml:/etc/otel/otel-collector.yaml
    ports:
      - "4317:4317"
      - "11308:11308" # prometheus metrics export
      - "55680:55680" # default traces port
      - "55681:55681" # default metrics port
    depends_on:
      - jaeger

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - 9090:9090
    command: "--config.file=/etc/prometheus/prometheus.yaml"
    volumes:
      - "./prometheus.yaml:/etc/prometheus/prometheus.yaml:ro"
    depends_on:
      - otel-collector

  zookeeper:
    image: rocketcycle-kafka
    restart: always
    container_name: zookeeper
    ports:
      - "2181:2181"
      - "8080:8080"
      - "9870:9870" # jmx prometheus agent
    environment:
      KAFKA_PROPERTIES: |
        dataDir=/tmp/zookeeper
        # the port at which the clients will connect
        clientPort=2181
        # disable the per-ip limit on the number of connections since this is a non-production config
        maxClientCnxns=0
        # Disable the adminserver by default to avoid port conflicts.
        # Set the port to something non-conflicting if choosing to enable this
        admin.enableServer=true
        admin.serverPort=8080
    entrypoint:
      /bin/sh -c "echo \"$$KAFKA_PROPERTIES\" > /opt/kafka/config/zookeeper.properties; /bin/bash start.sh zookeeper"

  broker:
    image: rocketcycle-kafka
    restart: always
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9871:9871" # jmx prometheus agent
    environment:
      KAFKA_PROPERTIES: |
        ############################# Server Basics #############################

        # The id of the broker. This must be set to a unique integer for each broker.
        broker.id=0

        ############################# Socket Server Settings #############################

        # The address the socket server listens on. It will get the value returned from
        # java.net.InetAddress.getCanonicalHostName() if not configured.
        #   FORMAT:
        #     listeners = listener_name://host_name:port
        #   EXAMPLE:
        #     listeners = PLAINTEXT://your.host.name:9092
        #listeners=PLAINTEXT://:9092

        # Hostname and port the broker will advertise to producers and consumers. If not set,
        # it uses the value for "listeners" if configured.  Otherwise, it will use the value
        # returned from java.net.InetAddress.getCanonicalHostName().
        advertised.listeners=PLAINTEXT://localhost:9092
        #jmx.port=9101

        # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
        #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

        # The number of threads that the server uses for receiving requests from the network and sending responses to the network
        num.network.threads=3

        # The number of threads that the server uses for processing requests, which may include disk I/O
        num.io.threads=8

        # The send buffer (SO_SNDBUF) used by the socket server
        socket.send.buffer.bytes=102400

        # The receive buffer (SO_RCVBUF) used by the socket server
        socket.receive.buffer.bytes=102400

        # The maximum size of a request that the socket server will accept (protection against OOM)
        socket.request.max.bytes=104857600


        ############################# Log Basics #############################

        # A comma separated list of directories under which to store log files
        log.dirs=/tmp/kafka-logs

        # The default number of log partitions per topic. More partitions allow greater
        # parallelism for consumption, but this will also result in more files across
        # the brokers.
        num.partitions=1

        # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
        # This value is recommended to be increased for installations with data dirs located in RAID array.
        num.recovery.threads.per.data.dir=1

        ############################# Internal Topic Settings  #############################
        # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
        # For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
        offsets.topic.replication.factor=1
        transaction.state.log.replication.factor=1
        transaction.state.log.min.isr=1

        ############################# Log Flush Policy #############################

        # Messages are immediately written to the filesystem but by default we only fsync() to sync
        # the OS cache lazily. The following configurations control the flush of data to disk.
        # There are a few important trade-offs here:
        #    1. Durability: Unflushed data may be lost if you are not using replication.
        #    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
        #    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
        # The settings below allow one to configure the flush policy to flush data after a period of time or
        # every N messages (or both). This can be done globally and overridden on a per-topic basis.

        # The number of messages to accept before forcing a flush of data to disk
        #log.flush.interval.messages=10000

        # The maximum amount of time a message can sit in a log before we force a flush
        #log.flush.interval.ms=1000

        ############################# Log Retention Policy #############################

        # The following configurations control the disposal of log segments. The policy can
        # be set to delete segments after a period of time, or after a given size has accumulated.
        # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
        # from the end of the log.

        # The minimum age of a log file to be eligible for deletion due to age
        log.retention.hours=168

        # A size-based retention policy for logs. Segments are pruned from the log unless the remaining
        # segments drop below log.retention.bytes. Functions independently of log.retention.hours.
        #log.retention.bytes=1073741824

        # The maximum size of a log segment file. When this size is reached a new log segment will be created.
        log.segment.bytes=1073741824

        # The interval at which log segments are checked to see if they can be deleted according
        # to the retention policies
        log.retention.check.interval.ms=300000

        ############################# Zookeeper #############################

        # Zookeeper connection string (see zookeeper docs for details).
        # This is a comma separated host:port pairs, each corresponding to a zk
        # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
        # You can also append an optional chroot string to the urls to specify the
        # root directory for all kafka znodes.
        zookeeper.connect=zookeeper:2181

        # Timeout in ms for connecting to zookeeper
        zookeeper.connection.timeout.ms=1000


        ############################# Group Coordinator Settings #############################

        # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
        # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
        # The default value for this is 3 seconds.
        # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
        # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
        group.initial.rebalance.delay.ms=0

        auto.create.topics.enable=false

    entrypoint:
      /bin/sh -c "echo \"$$KAFKA_PROPERTIES\" > /opt/kafka/config/server.properties; /bin/bash start.sh broker"
